{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a replication project for the paper, *Contrastive Learning of Medical Visual Represen- tations from Paired Images and Text* by Zhang et al., 2020."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The general results from the original paper were very much achieved and demonstrated in this replication project.\n",
    "- Performanceresultswereachievedatthesame level as the paper, and it was shown that the ConVIRT-based model outperformed the Imagenet-based model in deteremining the presence of COVID in the chest X-ray\n",
    "- It was also shown that the ConVIRT-based model achieved better results on 1% of RSNA than the ImageNet-based model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### MIMIC-CXR-JPG\n",
    "This dataset is entirely de- rived from MIMIC-CXR and the images are instead available in JPG format. It also includes the follow- ing metadata:\n",
    "- X-ray information such as procedure, position, image size (rows, columns), date and time, and others\n",
    "- Study labeling according to medical con- ditions, which include Atelectasis, Car- diomegaly, Pleural Effusion, and others (14 total)\n",
    "- Train / validation / test splits\n",
    "\n",
    "### COVIDx \n",
    "This is a public dataset containing chest X-rays available on Kaggle, made available along with COVIDNet (Wang et al., 2020). It can be downloaded from https://www.kaggle.com/datasets/andyczhao/covidx-cxr2. It is a continually growing dataset with 30,882 chest X-rays involving 17,026 patients as of this writing.\n",
    "\n",
    "### RSNA\n",
    "This is a public dataset also from Kaggle and used as the dataset for a past competition for pneumonia detection. It is available at https://www.kaggle.com/competitions/rsnapneumonia-detection-challenge/data. It contains 26,684 training images and 3,000 test images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVIDx\n",
    "\n",
    "Performance on COVIDx for both ImageNet and ConVIRT was at about the same level as in the original paper, with the ConVIRT-based model showing significant improvements over its ImageNet counterpart. \n",
    "\n",
    "Accuracy was at over 95% with ConVIRT using the entire dataset, and over 84% accuracy using 10% of the data. In the original paper, experiments were done on only 100% and 10% of the data, but with v3 of this dataset, which has more than double the size of that used in the original paper, I was able to run experiments on 1% of the data, achieving over 66% accuracy. Overall, training on COVIDx followed general expectations.\n",
    "\n",
    "The graph below shows a summary of the performance results on COVIDx."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![COVIDx Classification](covidx_classification.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSNA\n",
    "\n",
    "On the other hand, RSNA training with ImageNet and using parameters from the original paper was not as straightforward and overall results were poor. Performance on the entire dataset was only at 81.6 AUC compared to the original which was at 86.9 AUC, and is consistently lower for other dataset percentages than the original paper. However, the performance of the ConVIRT-based model still demonstrated a learning boost, largely confirming the impact of the ConVIRT contrastive learning strategy. For example, accuracy was shown to be much better on RSNA with just 1% of the data using ConVIRT compared to 100% of the data using ImageNet, and this was observable across all data subsets. \n",
    "\n",
    "The graph below displays a comparison of performance across data subsets for RSNA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RSNA](rsna_classification.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. [A simple framework for contrastive learning of visual representations](http://arxiv.org/abs/2002.05709).\n",
    "\n",
    "Phillip Lippe. 2023. [Tutorial 13: Self-supervised contrastive learning with simclr](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/13-contrastive-learning.html#SimCLR-implementation).\n",
    "\n",
    "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. [Representation learning with contrastive predictive coding](http://arxiv.org/abs/1807.03748).\n",
    "\n",
    "Linda Wang, Zhong Qiu Lin, and Alexander Wong. 2020. [Covid-net: a tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images](https://doi.org/10.1038/s41598-020-76550-z). *Scientific Reports*, 10(1):19549.\n",
    "\n",
    "Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. 2022. [Contrastive learning of medical visual representations from paired images and text](http://arxiv.org/abs/2010.00747)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uiuc-cs598-dlh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
